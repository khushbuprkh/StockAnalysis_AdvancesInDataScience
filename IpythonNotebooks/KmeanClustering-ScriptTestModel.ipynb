{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/mohan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/mohan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting h5py\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading h5py-2.7.0-cp27-cp27mu-manylinux1_x86_64.whl (4.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.4MB 289kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/dist-packages (from h5py)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from h5py)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-2.7.0\n"
     ]
    }
   ],
   "source": [
    "# !sudo pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Function for the Neural Network Done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import plotly\n",
    "import itertools\n",
    "from talib.abstract import *\n",
    "import os\n",
    "import sys\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.externals import joblib\n",
    "print(\"Import Function for the Neural Network Done\")\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# import plotly.graph_objs as go\n",
    "# plotly.tools.set_credentials_file(username='maiti.t', api_key='km8Kdfszic1Cu6ZAWiZJ')\n",
    "# plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'NeuralNetworkModel.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_open_normalised_prices(df, start, end):\n",
    "    \n",
    "    print('************************************************************************')\n",
    "    df1 = pd.DataFrame(df[\"High\"]/df[\"Open\"])\n",
    "    df1.columns=[\"H/O\"]\n",
    "    df1[\"L/O\"] = df[\"Low\"]/df[\"Open\"]\n",
    "    df1[\"C/O\"] = df[\"Close\"]/df[\"Open\"]\n",
    " \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_follow_cluster_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_follow_cluster_matrix(data):\n",
    "    print('************************************************************************')\n",
    "    data[\"ClusterTomorrow\"] = data[\"Cluster\"].shift(-1)\n",
    "    data.dropna(inplace=True)\n",
    "    data[\"ClusterTomorrow\"] = data[\"ClusterTomorrow\"].apply(int)\n",
    "#     print(\"Tesssssssssssttttttttttttttt\")\n",
    "#     print(zip(data[\"Cluster\"], data[\"ClusterTomorrow\"]))\n",
    "    data[\"ClusterMatrix\"] = list(zip(data[\"Cluster\"], data[\"ClusterTomorrow\"]))\n",
    "    cmvc = data[\"ClusterMatrix\"].value_counts()\n",
    "    clust_mat = np.zeros( (k, k) )\n",
    "    for row in cmvc.iteritems():\n",
    "#         print(row[1]*100.0/len(data))\n",
    "        clust_mat[row[0]] = row[1]*100.0/len(data)\n",
    "    print(\"Cluster Follow-on Matrix:\")\n",
    "    print(clust_mat)\n",
    "    return clust_mat,cmvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_indicators(stocks, period):\n",
    "#     for i in stocks:\n",
    "        print(\"***********************************************************************\")\n",
    "        stocks.columns = [s.lower() for s in stocks.columns]\n",
    "        print(\"Adding Features: Started  \")\n",
    "        features = pd.DataFrame(SMA(stocks, timeperiod=10))\n",
    "        features.columns = ['sma_10']\n",
    "        features['NATR']=pd.DataFrame(NATR(stocks, timeperiod=14))\n",
    "        features = pd.concat([features,STOCHF(stocks, fastk_period=14, fastd_period=3)], axis=1)\n",
    "        features['willr'] = pd.DataFrame(WILLR(stocks, timeperiod=14))\n",
    "        features['rsi'] = pd.DataFrame(RSI(stocks, timeperiod=14))\n",
    "        features['wma_10'] = pd.DataFrame(WMA(stocks,10))\n",
    "        features['T3'] =pd.DataFrame(T3(stocks, timeperiod=5, vfactor=0))\n",
    "        features['closePrice']=pd.DataFrame(stocks['close'].shift(-period))\n",
    "        features['return_pct_change'] = ROC(stocks, timeperiod=period)\n",
    "        features['return_pct_change'] = features['return_pct_change'].shift(-period)\n",
    "        features['pct_change'] = features['return_pct_change'].apply(lambda x: '1' if x > 0 else '0' if x <= 0 else np.nan)\n",
    "        features = features.dropna()\n",
    "           \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modellingNeuralNetwork(trainX,trainY,numbFeatures,i):\n",
    "    print(\"Creating Neural Network .....\")\n",
    "    starttime= time.time()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim=numbFeatures, activation='relu'))\n",
    "    model.add(Dense(11, input_dim=numbFeatures, activation='relu'))\n",
    "    model.add(Dense(11, input_dim=numbFeatures, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mae', 'mape'])\n",
    "    history1 = model.fit(trainX, trainY, epochs=20, batch_size=2, verbose=0)\n",
    "    print(\"Time Taken to Model %s seconds ---\" % (time.time() - starttime))\n",
    "    print(\"*******************************************\")\n",
    "    print(history1)\n",
    "    print(\"*******************************************\")\n",
    "    scores = model.evaluate(trainX,trainY,verbose = 0)\n",
    "#     scores = model.evaluate(X, Y, verbose=0)\n",
    "#     print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[1]*100))\n",
    "    print(model.summary())\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    h5filename = 'model'+str(i)+'.h5'\n",
    "    model.save_weights(h5filename)\n",
    "    print(\"Saved model to disk\")\n",
    "#     picklefilename = 'model'+str(i)+'.pkl'\n",
    "#     joblib.dump(model, picklefilename)\n",
    "#     joblib.dump(model, 'model'+str(i)+'.pkl')\n",
    "\n",
    "    return model,history1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset1(datasetReal,i):\n",
    "    print(type(datasetReal))\n",
    "    xdataset = datasetReal.copy()\n",
    "    xdataset = xdataset[xdataset.columns[xdataset.columns !='closePrice']]\n",
    "    dataset = np.array(xdataset)\n",
    "    print(\"Total Number of features in trainig Dataset \"+ str(xdataset.columns.size))\n",
    "    dataX = [dataset[n] for n in range(len(dataset)-i)]\n",
    "    return np.array(dataX),xdataset.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictedClosePrice2 = []\n",
    "frameshistory=[]\n",
    "# model = []\n",
    "def testingModel1(inputClusterone):\n",
    "    \n",
    "        starttime1= time.time()\n",
    "              \n",
    "        \n",
    "        for i in range(1,8):\n",
    "            print(\"Training the Neural Network for next \"+ str(i)+\" Prediction\")\n",
    "            starttime= time.time() \n",
    "            \n",
    "            print(\"Copying the input Dataset \")\n",
    "            inputdata = inputClusterone.copy()\n",
    "            \n",
    "            dataset1 = get_indicators(inputClusterone,i)\n",
    "            if(i ==1):\n",
    "                print(\"Getting the features for Test data \")\n",
    "                testdata = dataset1.tail(1)\n",
    "                joblib.dump(testdata, 'testdata2.pkl')\n",
    "\n",
    "            print(\"Making The Training Dataset\")\n",
    "            traindataset = dataset1.head(len(dataset1)-2)\n",
    "\n",
    "            trainY = np.array(traindataset['closePrice'])[i:]\n",
    "            trainX, numbFeatures = create_dataset1(traindataset,i)\n",
    "            \n",
    "#             m,h = modellingNeuralNetwork(trainX,trainY,numbFeatures,i)\n",
    "            \n",
    "            json_file = open('model'+str(i)+'.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            modelname = 'model'+str(i)+'.h5' \n",
    "            loaded_model.load_weights(modelname)\n",
    "            print(\"Loaded model\"+ str(i)+\" from disk\")\n",
    "\n",
    "            # evaluate loaded model on test data\n",
    "            loaded_model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mae', 'mape'])\n",
    "            \n",
    "#             f={\"models\":m}\n",
    "#             s = pickle.dumps( m, \"model\"+str(i)+\".pkl\" ) \n",
    "#             s = pickle.dumps(m) \n",
    "#             joblib.dump(s, 'model'+str(i)+'.pkl')\n",
    "            print(\"Model Compiled\")\n",
    "#             joblib.dump(m, '\"model\"+str(i)+\".pkl\"') \n",
    "#             model.append(m)\n",
    "            \n",
    "#             fullReport  = pd.DataFrame(h.history)\n",
    "#             fullReport.to_csv(\"Day_\"+ str(i)+\" NN_PredictionModel.csv\")\n",
    "#             finalMAPE=fullReport['mean_absolute_percentage_error'].tail(1)\n",
    "#             finalLoss = fullReport['loss'].tail(1)\n",
    "#             finalMAE = fullReport['mean_absolute_error'].tail(1)\n",
    "\n",
    "#             finalreport = pd.DataFrame(finalMAPE)\n",
    "#             finalreport.columns = ['MAPE']\n",
    "#             finalreport['Loss']=finalLoss\n",
    "#             finalreport['MAE']=finalMAE\n",
    "#             finalreport.index = range(1,len(finalreport)+1)\n",
    "\n",
    "#             frameshistory.append(finalreport)\n",
    "#             print(finalreport)\n",
    "\n",
    "            print(\"Testing on the Latest Trade Data ...\")\n",
    "\n",
    "#             trainfeature=traindataset.tail(len(traindataset)-1)\n",
    "#             trainlabel= trainfeature['closePrice']\n",
    "\n",
    "#             prediction = m.predict(trainX)\n",
    "#             my_list = map(lambda x: x[0], prediction)\n",
    "#             traintestlabel = pd.Series(my_list)\n",
    "\n",
    "#             testdata = dataset1.tail(1)\n",
    "            testdata = joblib.load('testdata2.pkl')\n",
    "#             testdata = testDataLatestTrade\n",
    "#             testexpectedlabel = testdata['closePrice']\n",
    "\n",
    "            xtest = testdata[testdata.columns[testdata.columns !='closePrice']]\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(\"Test record Given\")\n",
    "            print(xtest)\n",
    "            print(\"----------------------------------\")\n",
    "            \n",
    "            predictiontest = loaded_model.predict(np.array(xtest))\n",
    "            \n",
    "            \n",
    "            temp_list = map(lambda x: x[0], predictiontest)\n",
    "            testlabel = pd.Series(temp_list)\n",
    "            print(\"**********************************************************\")\n",
    "#             print(\"Excpected Close Price \"+ str( testexpectedlabel.values))\n",
    "            print(\"Predicted Close Price  \"+ str(testlabel[0]))\n",
    "#             predictedClosePrice.append(testlabel[0])\n",
    "            clf = joblib.load('predictedClosePrice2.pkl') \n",
    "            clf.append(testlabel[0])\n",
    "            joblib.dump(clf, 'predictedClosePrice2.pkl')\n",
    "            print(\"**********************************************************\")\n",
    "            print(\"Time Taken to Model %s seconds ---\" % (time.time() - starttime))\n",
    "            \n",
    "        print(\"Testing the Model Finished ...\")\n",
    "        print(\"Total Time Taken to Model %s seconds ---\" % (time.time() - starttime1))\n",
    "#         allrecord = pd.concat(frameshistory)\n",
    "#         allrecord.index = range(1,len(allrecord)+1)\n",
    "#         print(\"Final Model Evaluation Scores \")\n",
    "#         print(allrecord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datadownload(start,end):\n",
    "    try:\n",
    "        dow = web.DataReader(\"^DJI\", \"yahoo\", start, end)\n",
    "        dow.to_csv(\"AllDJIdata.csv\")\n",
    "    except (RuntimeError, TypeError, NameError,Exception):\n",
    "                   print(\"Oops! Yahoo Data Service is not working ....\")\n",
    "                   print(\"Data Taken from last updated repository\")\n",
    "                   dow=pd.read_csv(\"AllDJIdata.csv\")\n",
    "        \n",
    "    \n",
    "    return dow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(dow_norm,k):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(dow_norm)\n",
    "    labels = km.labels_\n",
    "    return labels,km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getinformationCluster(cmvc,km):\n",
    "    print(\"Ranking of Clusters \");\n",
    "    print(str(cmvc))\n",
    "    print('************************************************************************')\n",
    "    print(\"Centroid for the K Means Clusters \")\n",
    "    kmean = pd.DataFrame(km.cluster_centers_)\n",
    "    kmean.columns=['X','Y','Z']\n",
    "    print(kmean)\n",
    "    print('************************************************************************')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictCluster(km,dow_norm,cmvc,clust_mat):\n",
    "    print(\"Getting the Latest Trade OHLC Values for DOW 30 \") \n",
    "    testdoeN = dow_norm.copy()\n",
    "    print(\"*****************************************8\")\n",
    "#     print(testdoeN.tail(5))\n",
    "#     del testdoeN['Cluster']\n",
    "#     print(\"Testtttttt\")\n",
    "    \n",
    "    print(testdoeN.tail(1).values.reshape(-1,3))\n",
    "    predictedCluster = km.predict(testdoeN.tail(1).values.reshape(-1,3))[0]\n",
    "    print(\"Predicted Cluster of Last Trade OHLC Date \"+ \" \"+ str(testdoeN.index[len(testdoeN)-1])+\" \"+ str(predictedCluster))\n",
    "    predictedNextCluster = []\n",
    "    for i,row in cmvc.iteritems():\n",
    "        if(i[0]==predictedCluster):\n",
    "            predictedNextCluster.append(i[1])\n",
    "    print(\"Predicted Next Day Cluster would be : \"+ str(predictedNextCluster))\n",
    "    \n",
    "    probabCluster=[]\n",
    "    totalPrbablityfromPast=0\n",
    "    for i in predictedNextCluster:\n",
    "            totalPrbablityfromPast=totalPrbablityfromPast+clust_mat[predictedCluster][i]\n",
    "    #         probabCluster.append(clust_mat[predictedCluster][i])\n",
    "    print(\"Calulating the Relative Probablity \")\n",
    "    for i in predictedNextCluster:\n",
    "            probabCluster.append(clust_mat[predictedCluster][i]/totalPrbablityfromPast*100)\n",
    "            \n",
    "    nextClusterMatrix = pd.DataFrame(predictedNextCluster)\n",
    "    nextClusterMatrix.columns=[\"Cluster\"]\n",
    "    nextClusterMatrix['Probability']=probabCluster\n",
    "    \n",
    "    print(\"Next Day CLuster and its Relative probablity from the Predicted Cluster : \"+ str(predictedCluster))\n",
    "    print(nextClusterMatrix)\n",
    "    return nextClusterMatrix\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRightClusteredData(nextClusterMatrix,dowtrain):\n",
    "    print(\"Getting the filtered Past data where the maximum predicted range of the next DAY OHLC would be.. \")\n",
    "    inputClusterone = dowtrain[dowtrain['Cluster']==nextClusterMatrix['Cluster'][0]]\n",
    "    return inputClusterone\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas_datareader.data as web\n",
    "# start = datetime.datetime(2005, 1, 1)\n",
    "# end =  time.strftime(\"%Y-%m-%d\")\n",
    "# dow = web.DataReader(\"^DJI\", \"yahoo\", start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dow=pd.read_csv(\"AllDJIdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frameshistory=[]\n",
    "# def testingModel(i):\n",
    "    \n",
    "            \n",
    "              \n",
    "        \n",
    "# #         for i in range(1,3):\n",
    "#             print(\"Training the Neural Network for next \"+ str(i)+\" Prediction\")\n",
    "#             starttime= time.time() \n",
    "            \n",
    "#             print(\"Copying the input Dataset \")\n",
    "#             inputdata = inputClusterone.copy()\n",
    "            \n",
    "#             dataset1 = get_indicators(inputClusterone,i)\n",
    "\n",
    "#             print(\"Making The Training Dataset\")\n",
    "#             traindataset = dataset1.head(len(dataset1)-2)\n",
    "\n",
    "#             trainY = np.array(traindataset['closePrice'])[i:]\n",
    "#             trainX, numbFeatures = create_dataset1(traindataset,i)\n",
    "            \n",
    "#             m,h = modellingNeuralNetwork(trainX,trainY,numbFeatures,i)\n",
    "            \n",
    "# #             f={\"models\":m}\n",
    "# #             s = pickle.dumps( m, \"model\"+str(i)+\".pkl\" ) \n",
    "# #             s = pickle.dumps(m) \n",
    "            \n",
    "#             print(\"Model Saved\")\n",
    "# #             joblib.dump(m, '\"model\"+str(i)+\".pkl\"') \n",
    "# #             model.append(m)\n",
    "            \n",
    "#             fullReport  = pd.DataFrame(h.history)\n",
    "# #             fullReport.to_csv(\"Day_\"+ str(i)+\" NN_PredictionModel.csv\")\n",
    "#             finalMAPE=fullReport['mean_absolute_percentage_error'].tail(1)\n",
    "#             finalLoss = fullReport['loss'].tail(1)\n",
    "#             finalMAE = fullReport['mean_absolute_error'].tail(1)\n",
    "\n",
    "#             finalreport = pd.DataFrame(finalMAPE)\n",
    "#             finalreport.columns = ['MAPE']\n",
    "#             finalreport['Loss']=finalLoss\n",
    "#             finalreport['MAE']=finalMAE\n",
    "#             finalreport.index = range(1,len(finalreport)+1)\n",
    "\n",
    "#             frameshistory.append(finalreport)\n",
    "#             print(finalreport)\n",
    "\n",
    "#             print(\"Testing on the Latest Trade Data ...\")\n",
    "\n",
    "# #             trainfeature=traindataset.tail(len(traindataset)-1)\n",
    "# #             trainlabel= trainfeature['closePrice']\n",
    "\n",
    "# #             prediction = m.predict(trainX)\n",
    "# #             my_list = map(lambda x: x[0], prediction)\n",
    "# #             traintestlabel = pd.Series(my_list)\n",
    "\n",
    "#             testdata = dataset1.tail(1)\n",
    "# #             testdata = testDataLatestTrade\n",
    "# #             testexpectedlabel = testdata['closePrice']\n",
    "\n",
    "#             xtest = testdata[testdata.columns[testdata.columns !='closePrice']]\n",
    "#             predictiontest = m.predict(np.array(xtest))\n",
    "            \n",
    "            \n",
    "#             temp_list = map(lambda x: x[0], predictiontest)\n",
    "#             testlabel = pd.Series(temp_list)\n",
    "#             print(\"**********************************************************\")\n",
    "# #             print(\"Excpected Close Price \"+ str( testexpectedlabel.values))\n",
    "#             print(\"Predicted Close Price  \"+ str(testlabel[0]))\n",
    "    \n",
    "#             clf = joblib.load('predictedClosePrice2.pkl') \n",
    "#             clf.append(testlabel[0])\n",
    "#             joblib.dump(clf, 'predictedClosePrice2.pkl')\n",
    "# #             predictedClosePrice.append(testlabel[0])\n",
    "#             print(\"**********************************************************\")\n",
    "#             print(\"Time Taken to Model %s seconds ---\" % (time.time() - starttime))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************\n",
      "Step 1:Downloading the DOW 30 data from Yahoo Finance \n",
      "Status ::Data Download Finish....\n",
      "************************************************************************\n",
      "Step 2:Normalising the Data with respect to Daily Open Stock Price \n",
      "************************************************************************\n",
      "Number of Chosen Cluster : 5 \n",
      "************************************************************************\n",
      "Step 3: KNN Process Started ...\n",
      "Status : KNN Process Finished ...\n",
      "************************************************************************\n",
      "Cluster Follow-on Matrix:\n",
      "[[  5.63070148   4.62409563  14.21830764   0.2831079    0.66058509]\n",
      " [  5.37905002   2.67379679   5.94526581   0.44039006   0.72349796]\n",
      " [ 13.33752752   6.85750236  33.9100346    0.2831079    0.78641082]\n",
      " [  0.34602076   0.34602076   0.12582573   0.15728216   0.44039006]\n",
      " [  0.72349796   0.66058509   0.97514942   0.25165146   0.22019503]]\n",
      "************************************************************************\n",
      "Ranking of Clusters \n",
      "(2, 2)    1078\n",
      "(0, 2)     452\n",
      "(2, 0)     424\n",
      "(2, 1)     218\n",
      "(1, 2)     189\n",
      "(0, 0)     179\n",
      "(1, 0)     171\n",
      "(0, 1)     147\n",
      "(1, 1)      85\n",
      "(4, 2)      31\n",
      "(2, 4)      25\n",
      "(1, 4)      23\n",
      "(4, 0)      23\n",
      "(0, 4)      21\n",
      "(4, 1)      21\n",
      "(3, 4)      14\n",
      "(1, 3)      14\n",
      "(3, 1)      11\n",
      "(3, 0)      11\n",
      "(2, 3)       9\n",
      "(0, 3)       9\n",
      "(4, 3)       8\n",
      "(4, 4)       7\n",
      "(3, 3)       5\n",
      "(3, 2)       4\n",
      "Name: ClusterMatrix, dtype: int64\n",
      "************************************************************************\n",
      "Centroid for the K Means Clusters \n",
      "          X         Y         Z\n",
      "0  1.011460  0.998464  1.008834\n",
      "1  1.002020  0.983373  0.987101\n",
      "2  1.003287  0.995592  0.999498\n",
      "3  1.003179  0.950500  0.958893\n",
      "4  1.034475  0.996097  1.031392\n",
      "************************************************************************\n",
      "Getting the Latest Trade OHLC Values for DOW 30 \n",
      "*****************************************8\n",
      "[[ 1.00315163  0.99616799  0.99768141]]\n",
      "Predicted Cluster of Last Trade OHLC Date  2017-08-18 00:00:00 2\n",
      "Predicted Next Day Cluster would be : [2, 0, 1, 4, 3]\n",
      "Calulating the Relative Probablity \n",
      "Next Day CLuster and its Relative probablity from the Predicted Cluster : 2\n",
      "   Cluster  Probability\n",
      "0        2    61.459521\n",
      "1        0    24.173318\n",
      "2        1    12.428734\n",
      "3        4     1.425314\n",
      "4        3     0.513113\n",
      "Getting the filtered Past data where the maximum predicted range of the next DAY OHLC would be.. \n",
      "Training the Neural Network for next 1 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n",
      "Getting the features for Test data \n",
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model1 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  22474.2265625\n",
      "**********************************************************\n",
      "Time Taken to Model 0.13171505928 seconds ---\n",
      "Training the Neural Network for next 2 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n",
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model2 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  22027.1386719\n",
      "**********************************************************\n",
      "Time Taken to Model 0.12415599823 seconds ---\n",
      "Training the Neural Network for next 3 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n",
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model3 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  22208.0527344\n",
      "**********************************************************\n",
      "Time Taken to Model 0.130833148956 seconds ---\n",
      "Training the Neural Network for next 4 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n",
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model4 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  21904.7871094\n",
      "**********************************************************\n",
      "Time Taken to Model 0.138615131378 seconds ---\n",
      "Training the Neural Network for next 5 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n",
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model5 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  22384.8964844\n",
      "**********************************************************\n",
      "Time Taken to Model 0.138999938965 seconds ---\n",
      "Training the Neural Network for next 6 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model6 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  22197.3144531\n",
      "**********************************************************\n",
      "Time Taken to Model 0.148769855499 seconds ---\n",
      "Training the Neural Network for next 7 Prediction\n",
      "Copying the input Dataset \n",
      "***********************************************************************\n",
      "Adding Features: Started  \n",
      "Making The Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Total Number of features in trainig Dataset 10\n",
      "Loaded model7 from disk\n",
      "Model Compiled\n",
      "Testing on the Latest Trade Data ...\n",
      "-----------------------------------------\n",
      "Test record Given\n",
      "                  sma_10      NATR      fastk      fastd      willr       rsi  \\\n",
      "Date                                                                            \n",
      "2017-08-15  22008.264063  0.474287  63.335255  57.544256 -36.664745  62.57257   \n",
      "\n",
      "                  wma_10            T3  return_pct_change pct_change  \n",
      "Date                                                                  \n",
      "2017-08-15  21986.315945  21966.189002           0.117637          1  \n",
      "----------------------------------\n",
      "**********************************************************\n",
      "Predicted Close Price  22250.8203125\n",
      "**********************************************************\n",
      "Time Taken to Model 0.15051984787 seconds ---\n",
      "Testing the Model Finished ...\n",
      "Total Time Taken to Model 0.966096162796 seconds ---\n",
      "**************************************\n",
      "[22474.2265625, 22027.138671875, 22208.052734375, 21904.787109375, 22384.896484375, 22197.314453125, 22250.8203125]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Obtain DOW 30  pricing data from Yahoo Finance\n",
    "    print('************************************************************************')\n",
    "    print(\"Step 1:Downloading the DOW 30 data from Yahoo Finance \")\n",
    "    start = datetime.datetime(2005, 1, 1)\n",
    "    end =  time.strftime(\"%Y-%m-%d\")\n",
    "#     dow = web.DataReader(\"^DJI\", \"yahoo\", start, end)\n",
    "    dow = datadownload(start,end)\n",
    "    print(\"Status ::Data Download Finish....\")\n",
    "    print('************************************************************************')\n",
    "    print(\"Step 2:Normalising the Data with respect to Daily Open Stock Price \")\n",
    "    \n",
    "    dow_norm = get_open_normalised_prices(dow, start, end)\n",
    "    print(\"Number of Chosen Cluster : 5 \")\n",
    "    \n",
    "    k = 5\n",
    "    print('************************************************************************')\n",
    "    print(\"Step 3: KNN Process Started ...\")\n",
    "    \n",
    "    # Deleting the last row from the KNN Dataset \n",
    "    traindowNorm = dow_norm.copy()\n",
    "    traindowNorm.drop(traindowNorm.index[len(traindowNorm)-1])\n",
    "    \n",
    "    # Reatining the latest Trade info as normalised one\n",
    "    testDataLatestTrade = dow_norm.tail(1)\n",
    "    \n",
    "    labels,km = knn(traindowNorm,k)\n",
    "    \n",
    "    dowtrain = dow.copy()\n",
    "    dowtrain.drop(dowtrain.index[len(dowtrain)-1])\n",
    "    dowtrain[\"Cluster\"] = labels\n",
    "    traindowNorm[\"Cluster\"] = labels\n",
    "    \n",
    "    print(\"Status : KNN Process Finished ...\")\n",
    "    # Create and output the cluster follow-on matrix\n",
    "    clust_mat,cmvc = create_follow_cluster_matrix(dowtrain)\n",
    "    \n",
    "    print('************************************************************************')\n",
    "    \n",
    "    getinformationCluster(cmvc,km)\n",
    "    \n",
    "    nextClusterMatrix = predictCluster(km,testDataLatestTrade,cmvc,clust_mat)\n",
    "    \n",
    "    inputClusterone = getRightClusteredData(nextClusterMatrix,dowtrain)\n",
    "    \n",
    "    inputClusterone.append(dow.tail(1))\n",
    "    \n",
    "    joblib.dump(predictedClosePrice2, 'predictedClosePrice2.pkl')\n",
    "#     p = Pool(20)\n",
    "# #     frameshistory=[]\n",
    "#     print(p.map(testingModel, range (1,4)))\n",
    "    testingModel1(inputClusterone)\n",
    "#     print(\"Testing the Model Finished ...\")\n",
    "#     allrecord = pd.concat(frameshistory)\n",
    "#     allrecord.index = range(1,len(allrecord)+1)\n",
    "#     print(\"Final Model Evaluation Scores \")\n",
    "#     print(allrecord)\n",
    "    # load json and create model\n",
    "    clf = joblib.load('predictedClosePrice2.pkl')\n",
    "    print(\"**************************************\")\n",
    "    print(clf)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('predictedClosePrice2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22474.2265625,\n",
       " 22027.138671875,\n",
       " 22208.052734375,\n",
       " 21904.787109375,\n",
       " 22384.896484375,\n",
       " 22197.314453125,\n",
       " 22250.8203125]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
